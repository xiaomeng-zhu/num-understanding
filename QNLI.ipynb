{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a864457b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/jielanzheng/anaconda3/lib/python3.11/site-packages/SQLAlchemy-1.4.23-py3.11-macosx-11.1-arm64.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: replicate in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (0.25.2)\n",
      "Requirement already satisfied: httpx<1,>=0.21.0 in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from replicate) (0.27.0)\n",
      "Requirement already satisfied: packaging in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from replicate) (23.0)\n",
      "Requirement already satisfied: pydantic>1.10.7 in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from replicate) (2.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from replicate) (4.7.1)\n",
      "Requirement already satisfied: anyio in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.21.0->replicate) (3.5.0)\n",
      "Requirement already satisfied: certifi in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.21.0->replicate) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.21.0->replicate) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.21.0->replicate) (3.4)\n",
      "Requirement already satisfied: sniffio in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.21.0->replicate) (1.2.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from pydantic>1.10.7->replicate) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/jielanzheng/anaconda3/lib/python3.11/site-packages (from pydantic>1.10.7->replicate) (2.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "891a70f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "REPLICATE_API_TOKEN = getpass()\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a2d2019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    }
   ],
   "source": [
    "## Load Data\n",
    "import json\n",
    "\n",
    "## NewsNLI\n",
    "with open(\"QNLI/NewsNLI.json\", 'r') as file:\n",
    "    NewsNLI = json.loads(file.read())\n",
    "    \n",
    "## AWPNLI\n",
    "with open(\"QNLI/AWPNLI.json\", 'r') as file:\n",
    "    AWPNLI = json.loads(file.read())\n",
    "    \n",
    "## RedditNLI\n",
    "with open(\"QNLI/RedditNLI.json\", 'r') as file:\n",
    "    RedditNLI = json.loads(file.read())\n",
    "\n",
    "## RTEQuant\n",
    "with open(\"QNLI/RTE_Quant.json\", 'r') as file:\n",
    "    RTEQuant = json.loads(file.read())\n",
    "\n",
    "## Stress Testing\n",
    "with open(\"QNLI/QNLI-Stress Test/QNLI-Stress Test_train.json\", 'r') as file:\n",
    "    QNLIStress = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f8bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "from tqdm import tqdm\n",
    "\n",
    "def qnli_batch(dataset, model_name, result_name):\n",
    "    answers = {}\n",
    "    example_prompt = f\"\"\"\n",
    "        Given statement 1 and statement 2, choose between 2 options. Answer one word.\n",
    "        Example 1: \n",
    "        Statement1: {dataset[0]['statement1']}\n",
    "        Statement2: {dataset[0]['statement2']}\n",
    "        Options: {dataset[0]['options']}\n",
    "        Answer: {dataset[0]['answer']}\n",
    "        Example 2:\n",
    "        Statement1: {dataset[1]['statement1']}\n",
    "        Statement2: {dataset[1]['statement2']}\n",
    "        Options: {dataset[1]['options']}\n",
    "        Answer: {dataset[1]['answer']}\n",
    "        Based on the example, answer the following question:\n",
    "        \"\"\"\n",
    "    for i in tqdm(range(len(dataset)-2), desc=\"Processing Questions\"):\n",
    "        question_prompt = f\"\"\"\n",
    "            Statement1: {dataset[i+2]['statement1']}\n",
    "            Statement2: {dataset[i+2]['statement2']}\n",
    "            Options: {dataset[i+2]['options']}\n",
    "            \"\"\"\n",
    "        ans = ''\n",
    "        for event in replicate.stream(model_name, input={\"prompt\": example_prompt + question_prompt},):\n",
    "            ans += str(event)\n",
    "        answers[i+2] = ans\n",
    "    \n",
    "    with open(f'{result_name}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(answers, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90429ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name1 = \"meta/meta-llama-3-70b-instruct\"\n",
    "model_name2 = \"meta/llama-2-70b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aad75cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 966/966 [06:02<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch(NewsNLI, model_name1, 'NewsNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e9f298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 966/966 [35:41<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch(NewsNLI, model_name2, 'NewsNLIllama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "418106f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 720/720 [04:46<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch(AWPNLI, model_name1, 'AWPNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d668b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 720/720 [27:04<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch(AWPNLI, model_name2, 'AWPNLIllama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5211f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 248/248 [01:54<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch(RedditNLI, model_name1, 'RedditNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c086aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 248/248 [09:20<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch(RedditNLI, model_name2, 'RedditNLIllama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11eac93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 164/164 [00:54<00:00,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch(RTEQuant, model_name1, 'RTEQuantllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14c3f240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 164/164 [05:36<00:00,  2.05s/it]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch(RTEQuant, model_name2, 'RTEQuantllama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c37d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qnli_batch_sci(dataset, model_name, result_name):\n",
    "    answers = {}\n",
    "    example_prompt = f\"\"\"\n",
    "        Given statement 1 and statement 2, choose between 2 options. Answer one word.\n",
    "        Example 1: \n",
    "        Statement1: {dataset[0]['statement1_sci_10E']}\n",
    "        Statement2: {dataset[0]['statement2_sci_10E']}\n",
    "        Options: {dataset[0]['options']}\n",
    "        Answer: {dataset[0]['answer']}\n",
    "        Example 2:\n",
    "        Statement1: {dataset[1]['statement1_sci_10E']}\n",
    "        Statement2: {dataset[1]['statement2_sci_10E']}\n",
    "        Options: {dataset[1]['options']}\n",
    "        Answer: {dataset[1]['answer']}\n",
    "        Based on the example, answer the following question:\n",
    "        \"\"\"\n",
    "    for i in tqdm(range(len(dataset)-2), desc=\"Processing Questions\"):\n",
    "        question_prompt = f\"\"\"\n",
    "            Statement1: {dataset[i+2]['statement1_sci_10E']}\n",
    "            Statement2: {dataset[i+2]['statement2_sci_10E']}\n",
    "            Options: {dataset[i+2]['options']}\n",
    "            \"\"\"\n",
    "        ans = ''\n",
    "        for event in replicate.stream(model_name, input={\"prompt\": example_prompt + question_prompt},):\n",
    "            ans += str(event)\n",
    "        answers[i+2] = ans\n",
    "    \n",
    "    with open(f'{result_name}_sci.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(answers, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cccfa209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 966/966 [07:22<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch_sci(NewsNLI, model_name1, 'NewsNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2feae35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 720/720 [04:34<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch_sci(AWPNLI, model_name1, 'AWPNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2027f46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 248/248 [01:35<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch_sci(RedditNLI, model_name1, 'RedditNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "653ff99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 164/164 [00:55<00:00,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch_sci(RTEQuant, model_name1, 'RTEQuantllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3523c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qnli_batch_masked(dataset, model_name, result_name):\n",
    "    answers = {}\n",
    "    example_prompt = f\"\"\"\n",
    "        Given statement 1 and statement 2, choose between 2 options. Answer one word.\n",
    "        Example 1: \n",
    "        Statement1: {dataset[0]['statement1_mask']}\n",
    "        Statement2: {dataset[0]['statement2_mask']}\n",
    "        Options: {dataset[0]['options']}\n",
    "        Answer: {dataset[0]['answer']}\n",
    "        Example 2:\n",
    "        Statement1: {dataset[1]['statement1_mask']}\n",
    "        Statement2: {dataset[1]['statement2_mask']}\n",
    "        Options: {dataset[1]['options']}\n",
    "        Answer: {dataset[1]['answer']}\n",
    "        Based on the example, answer the following question:\n",
    "        \"\"\"\n",
    "    for i in tqdm(range(len(dataset)-2), desc=\"Processing Questions\"):\n",
    "        question_prompt = f\"\"\"\n",
    "            Statement1: {dataset[i+2]['statement1_mask']}\n",
    "            Statement2: {dataset[i+2]['statement2_mask']}\n",
    "            Options: {dataset[i+2]['options']}\n",
    "            \"\"\"\n",
    "        ans = ''\n",
    "        for event in replicate.stream(model_name, input={\"prompt\": example_prompt + question_prompt},):\n",
    "            ans += str(event)\n",
    "        answers[i+2] = ans\n",
    "    \n",
    "    with open(f'{result_name}_masked.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(answers, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "109a446d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 966/966 [07:04<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch_masked(NewsNLI, model_name1, 'NewsNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f746992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 720/720 [05:09<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch_masked(AWPNLI, model_name1, 'AWPNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b459ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 248/248 [02:27<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch_masked(RedditNLI, model_name1, 'RedditNLIllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f1e2116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Questions: 100%|███████████████████| 164/164 [01:12<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "qnli_batch_masked(RTEQuant, model_name1, 'RTEQuantllama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cce42055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_qnli(results, dataset):\n",
    "    \n",
    "    all_labels = set(item['answer'].lower().strip() for item in dataset)\n",
    "    confusion_matrix = {label: {'tp': 0, 'fp': 0, 'fn': 0} for label in all_labels}\n",
    "    \n",
    "    # Populate the confusion matrix\n",
    "    for i in range(len(dataset)-2):\n",
    "        \n",
    "        true_label = dataset[i+2]['answer'].lower().strip()\n",
    "        predicted_label = results[f'{i+2}'].strip().lower()\n",
    "        \n",
    "        for label in all_labels:\n",
    "            if true_label == label:\n",
    "                if label in predicted_label:\n",
    "                    confusion_matrix[label]['tp'] += 1\n",
    "                else:\n",
    "                    confusion_matrix[label]['fn'] += 1\n",
    "            else:\n",
    "                if label in predicted_label:\n",
    "                    confusion_matrix[label]['fp'] += 1\n",
    "\n",
    "    # Calculate precision, recall, and F1 score for each class\n",
    "    scores = {}\n",
    "    for label, counts in confusion_matrix.items():\n",
    "        precision = counts['tp'] / (counts['tp'] + counts['fp']) if (counts['tp'] + counts['fp']) > 0 else 0\n",
    "        recall = counts['tp'] / (counts['tp'] + counts['fn']) if (counts['tp'] + counts['fn']) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        scores[label] = {'precision': precision, 'recall': recall, 'f1_score': f1_score}\n",
    "\n",
    "    # Calculate macro-average F1 score\n",
    "    macro_f1_score = sum(score['f1_score'] for score in scores.values()) / len(scores)\n",
    "\n",
    "    return scores, macro_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a5d6033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'neutral': {'precision': 0.4971687429218573, 'recall': 0.9242105263157895, 'f1_score': 0.646539027982327}, 'entailment': {'precision': 0.6039603960396039, 'recall': 0.2484725050916497, 'f1_score': 0.35209235209235207}}, 0.49931569003733955)\n",
      "({'neutral': {'precision': 0.8944954128440367, 'recall': 0.4105263157894737, 'f1_score': 0.5627705627705628}, 'entailment': {'precision': 0.6256684491978609, 'recall': 0.9531568228105907, 'f1_score': 0.7554479418886199}}, 0.6591092523295914)\n",
      "({'neutral': {'precision': 0.8983957219251337, 'recall': 0.35368421052631577, 'f1_score': 0.5075528700906344}, 'entailment': {'precision': 0.6059050064184852, 'recall': 0.9613034623217923, 'f1_score': 0.7433070866141732}}, 0.6254299783524038)\n",
      "({'neutral': {'precision': 0.8720379146919431, 'recall': 0.3873684210526316, 'f1_score': 0.5364431486880467}, 'entailment': {'precision': 0.614569536423841, 'recall': 0.945010183299389, 'f1_score': 0.7447833065810593}}, 0.640613227634553)\n"
     ]
    }
   ],
   "source": [
    "## NewsNLI\n",
    "with open(\"NewsNLIllama3.json\", 'r') as file:\n",
    "    NewsNLIllama3 = json.loads(file.read())\n",
    "with open(\"NewsNLIllama2.json\", 'r') as file:\n",
    "    NewsNLIllama2 = json.loads(file.read())\n",
    "    \n",
    "with open(\"NewsNLIllama3_sci.json\", 'r') as file:\n",
    "    NewsNLIllama3_sci = json.loads(file.read())\n",
    "with open(\"NewsNLIllama3_masked.json\", 'r') as file:\n",
    "    NewsNLIllama3_masked = json.loads(file.read())\n",
    "\n",
    "print(evaluation_qnli(NewsNLIllama2, NewsNLI))\n",
    "print(evaluation_qnli(NewsNLIllama3, NewsNLI))\n",
    "print(evaluation_qnli(NewsNLIllama3_sci, NewsNLI))\n",
    "print(evaluation_qnli(NewsNLIllama3_masked, NewsNLI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d4368d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'entailment': {'precision': 0.9772727272727273, 'recall': 0.23822714681440443, 'f1_score': 0.3830734966592428}, 'contradiction': {'precision': 0.5013966480446927, 'recall': 1.0, 'f1_score': 0.6679069767441861}}, 0.5254902367017145)\n",
      "({'entailment': {'precision': 0.5336538461538461, 'recall': 0.9224376731301939, 'f1_score': 0.6761421319796954}, 'contradiction': {'precision': 0.7083333333333334, 'recall': 0.1894150417827298, 'f1_score': 0.29890109890109895}}, 0.4875216154403972)\n",
      "({'entailment': {'precision': 0.524031007751938, 'recall': 0.9362880886426593, 'f1_score': 0.6719681908548708}, 'contradiction': {'precision': 0.6933333333333334, 'recall': 0.14484679665738162, 'f1_score': 0.23963133640552994}}, 0.45579976363020036)\n",
      "({'entailment': {'precision': 0.5015432098765432, 'recall': 0.9002770083102493, 'f1_score': 0.6442021803766105}, 'contradiction': {'precision': 0.5, 'recall': 0.10027855153203342, 'f1_score': 0.16705336426914152}}, 0.405627772322876)\n"
     ]
    }
   ],
   "source": [
    "## AWPNLI\n",
    "with open(\"AWPNLIllama2.json\", 'r') as file:\n",
    "    AWPNLIllama2 = json.loads(file.read())\n",
    "with open(\"AWPNLIllama3.json\", 'r') as file:\n",
    "    AWPNLIllama3 = json.loads(file.read())\n",
    "    \n",
    "with open(\"AWPNLIllama3_sci.json\", 'r') as file:\n",
    "    AWPNLIllama3_sci = json.loads(file.read())\n",
    "with open(\"AWPNLIllama3_masked.json\", 'r') as file:\n",
    "    AWPNLIllama3_masked = json.loads(file.read())\n",
    "\n",
    "print(evaluation_qnli(AWPNLIllama2, AWPNLI))\n",
    "print(evaluation_qnli(AWPNLIllama3, AWPNLI))\n",
    "print(evaluation_qnli(AWPNLIllama3_sci, AWPNLI))\n",
    "print(evaluation_qnli(AWPNLIllama3_masked, AWPNLI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e10e6676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'neutral': {'precision': 0.625, 'recall': 0.17857142857142858, 'f1_score': 0.2777777777777778}, 'entailment': {'precision': 0.6994219653179191, 'recall': 0.8287671232876712, 'f1_score': 0.7586206896551725}, 'contradiction': {'precision': 0.171875, 'recall': 0.6111111111111112, 'f1_score': 0.2682926829268293}}, 0.4348970501199265)\n",
      "({'neutral': {'precision': 0.775, 'recall': 0.36904761904761907, 'f1_score': 0.5000000000000001}, 'entailment': {'precision': 0.7287234042553191, 'recall': 0.9383561643835616, 'f1_score': 0.8203592814371258}, 'contradiction': {'precision': 0.4, 'recall': 0.4444444444444444, 'f1_score': 0.4210526315789474}}, 0.5804706376720244)\n",
      "({'neutral': {'precision': 0.6346153846153846, 'recall': 0.39285714285714285, 'f1_score': 0.4852941176470588}, 'entailment': {'precision': 0.7380952380952381, 'recall': 0.8493150684931506, 'f1_score': 0.7898089171974523}, 'contradiction': {'precision': 0.2857142857142857, 'recall': 0.4444444444444444, 'f1_score': 0.34782608695652173}}, 0.5409763739336776)\n",
      "({'neutral': {'precision': 0.75, 'recall': 0.32142857142857145, 'f1_score': 0.45000000000000007}, 'entailment': {'precision': 0.708994708994709, 'recall': 0.9178082191780822, 'f1_score': 0.7999999999999999}, 'contradiction': {'precision': 0.391304347826087, 'recall': 0.5, 'f1_score': 0.4390243902439025}}, 0.5630081300813008)\n"
     ]
    }
   ],
   "source": [
    "## RedditNLI\n",
    "with open(\"RedditNLIllama2.json\", 'r') as file:\n",
    "    RedditNLIllama2 = json.loads(file.read())\n",
    "with open(\"RedditNLIllama3.json\", 'r') as file:\n",
    "    RedditNLIllama3 = json.loads(file.read())\n",
    "    \n",
    "with open(\"RedditNLIllama3_sci.json\", 'r') as file:\n",
    "    RedditNLIllama3_sci = json.loads(file.read())\n",
    "with open(\"RedditNLIllama3_masked.json\", 'r') as file:\n",
    "    RedditNLIllama3_masked = json.loads(file.read())\n",
    "\n",
    "print(evaluation_qnli(RedditNLIllama2, RedditNLI))\n",
    "print(evaluation_qnli(RedditNLIllama3, RedditNLI))\n",
    "print(evaluation_qnli(RedditNLIllama3_sci, RedditNLI))\n",
    "print(evaluation_qnli(RedditNLIllama3_masked, RedditNLI))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cfeed8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'neutral': {'precision': 0.9285714285714286, 'recall': 0.2736842105263158, 'f1_score': 0.4227642276422765}, 'entailment': {'precision': 0.48905109489051096, 'recall': 0.9710144927536232, 'f1_score': 0.6504854368932038}}, 0.5366248322677402)\n",
      "({'neutral': {'precision': 0.9833333333333333, 'recall': 0.6210526315789474, 'f1_score': 0.7612903225806452}, 'entailment': {'precision': 0.6538461538461539, 'recall': 0.9855072463768116, 'f1_score': 0.7861271676300579}}, 0.7737087451053515)\n",
      "({'neutral': {'precision': 0.9333333333333333, 'recall': 0.5894736842105263, 'f1_score': 0.7225806451612904}, 'entailment': {'precision': 0.625, 'recall': 0.9420289855072463, 'f1_score': 0.7514450867052023}}, 0.7370128659332463)\n",
      "({'neutral': {'precision': 1.0, 'recall': 0.49473684210526314, 'f1_score': 0.6619718309859155}, 'entailment': {'precision': 0.5897435897435898, 'recall': 1.0, 'f1_score': 0.7419354838709677}}, 0.7019536574284416)\n"
     ]
    }
   ],
   "source": [
    "## RTEQuant\n",
    "with open(\"RTEQuantllama2.json\", 'r') as file:\n",
    "    RTEQuantllama2 = json.loads(file.read())\n",
    "with open(\"RTEQuantllama3.json\", 'r') as file:\n",
    "    RTEQuantllama3 = json.loads(file.read())\n",
    "    \n",
    "with open(\"RTEQuantllama3_sci.json\", 'r') as file:\n",
    "    RTEQuantllama3_sci = json.loads(file.read())\n",
    "with open(\"RTEQuantllama3_masked.json\", 'r') as file:\n",
    "    RTEQuantllama3_masked = json.loads(file.read())\n",
    "\n",
    "print(evaluation_qnli(RTEQuantllama2, RTEQuant))\n",
    "print(evaluation_qnli(RTEQuantllama3, RTEQuant))\n",
    "print(evaluation_qnli(RTEQuantllama3_sci, RTEQuant))\n",
    "print(evaluation_qnli(RTEQuantllama3_masked, RTEQuant))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
